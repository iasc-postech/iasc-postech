import copy
from re import M
from typing import Optional, List, Tuple
from matplotlib.transforms import Bbox
import torch
import torch.nn.functional as F
from torch import nn, Tensor
import math
from einops import rearrange, repeat
import warnings

from torch.nn.init import _calculate_fan_in_and_fan_out

def init_weights(m):
    if isinstance(m, nn.Linear):
        trunc_normal_(m.weight, std=0.02)
        if isinstance(m, nn.Linear) and m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.LayerNorm):
        nn.init.constant_(m.bias, 0)
        nn.init.constant_(m.weight, 1.0)

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    if mode == 'fan_in':
        denom = fan_in
    elif mode == 'fan_out':
        denom = fan_out
    elif mode == 'fan_avg':
        denom = (fan_in + fan_out) / 2

    variance = scale / denom

    if distribution == "truncated_normal":
        # constant is stddev of standard normal truncated to (-2, 2)
        trunc_normal_(tensor, std=math.sqrt(variance) / .87962566103423978)
    elif distribution == "normal":
        tensor.normal_(std=math.sqrt(variance))
    elif distribution == "uniform":
        bound = math.sqrt(3 * variance)
        tensor.uniform_(-bound, bound)
    else:
        raise ValueError(f"invalid distribution {distribution}")

def lecun_normal_(tensor):
    variance_scaling_(tensor, mode='fan_in', distribution='truncated_normal')

def _get_activation_fn(activation):
    """Return an activation function given a string"""
    if activation == "relu":
        return F.relu
    if activation == "gelu":
        return F.gelu
    if activation == "glu":
        return F.glu
    raise RuntimeError(F"activation should be relu/gelu, not {activation}.")

def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])

class ObjInferTransformer_type_1(nn.Module):
    def __init__(self, encoder_layer, num_layers, norm=None, no_pe=False, pe_type='abs4c', no_classifier=True, dim=256, class_numb=200, contain_is_thing=True):
        super().__init__()
        self.layers = _get_clones(encoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm
        self.no_pe = no_pe
        self.pe_type = pe_type
        self.no_classifier = no_classifier
        self.dim = dim
        self.class_numb = class_numb

        self.missing_token = nn.Parameter(torch.randn(1, dim), requires_grad=True)
        self.contain_is_thing = contain_is_thing

        if self.contain_is_thing:
            if self.pe_type in ['abs4c', 'rel4c']:
                self.pos_embd_layer = nn.Sequential(nn.Linear(5, int(dim/2)), nn.ReLU(), nn.Linear(int(dim/2), dim), nn.Sigmoid())
            elif self.pe_type in ['abs2c', 'rel2c']:
                self.pos_embd_layer = nn.Sequential(nn.Linear(3, int(dim/2)), nn.ReLU(), nn.Linear(int(dim/2), dim), nn.Sigmoid())

        else:

            if self.pe_type in ['abs4c', 'rel4c']:
                self.pos_embd_layer = nn.Sequential(nn.Linear(4, int(dim/2)), nn.ReLU(), nn.Linear(int(dim/2), dim), nn.Sigmoid())
            elif self.pe_type in ['abs2c', 'rel2c']:
                self.pos_embd_layer = nn.Sequential(nn.Linear(2, int(dim/2)), nn.ReLU(), nn.Linear(int(dim/2), dim), nn.Sigmoid())
            elif self.pe_type in ['learn']:
                self.learn_pe4detected = nn.Parameter(torch.randn(100, dim), requires_grad=True)
                self.learn_pe4missing = nn.Parameter(torch.randn(1, dim), requires_grad=True)

        self.label_embd_layer = nn.Embedding(num_embeddings=self.class_numb, embedding_dim=256)
        
        if no_classifier:
            self.final_token_prediction = nn.Sequential(*[
                nn.Linear(in_features=self.dim, out_features=self.dim),
                nn.GELU(),
                nn.LayerNorm(self.dim, eps=1e-12)
            ])
            # self.final_bias = nn.Parameter(torch.zeros(128, 128, self.class_numb))

        trunc_normal_(self.missing_token)
        self.apply(init_weights)

    def forward(self, detected_obj_label, bboxes, missing_box, is_thing_label,
                key_padding_mask,
                attn_mask: Optional[Tensor] = None):
        
        attn_matrix_list = []

        detected_obj_feature = self.label_embd_layer(detected_obj_label)

        # mask = repeat(mask, 'B T S -> B nhead T S', nhead=self.layers[0].nhead)
        # mask = rearrange(mask, 'B nhead T S -> (B nhead) T S')

        missing_box = repeat(missing_box, 'B coord -> B 1 coord')
        missing_token = repeat(self.missing_token, 'single E -> batch single E', batch=detected_obj_feature.size(0))

        if self.no_pe:
            detected_obj_pos = None
            missing_obj_pos = None
        else:
            if self.pe_type in ['abs2c', 'rel2c']:
                bboxes = bboxes[:, :, :2]
                missing_box = missing_box[:, :, :2]

                if self.pe_type in ['rel2c']:
                    bboxes -= missing_box
                    missing_box -= missing_box
            
            if self.pe_type in ['rel4c']:
                wh_bboxes = bboxes[:, :, 2:]
                wh_missing_box = missing_box[:, :, 2:]

                cc_bboxes = bboxes[:, :, :2]
                cc_missing_box = missing_box[:, :, :2]
                
                cc_bboxes -= cc_missing_box
                cc_missing_box -= cc_missing_box

                bboxes = torch.cat([cc_bboxes, wh_bboxes],dim=2)
                missing_box = torch.cat([cc_missing_box, wh_missing_box],dim=2)
                            
            if self.contain_is_thing:
                bboxes = torch.cat([bboxes, is_thing_label.unsqueeze(dim=-1).type_as(bboxes)], dim=2)
                missing_box = torch.cat([missing_box, torch.tensor([[[1]]]).repeat(missing_box.shape[0],1,1).type_as(missing_box)], dim=2)

            if self.pe_type in ['learn']:
                detected_obj_pos = self.learn_pe4detected
                missing_obj_pos = self.learn_pe4missing
            else:
                detected_obj_pos = self.pos_embd_layer(bboxes)
                missing_obj_pos = self.pos_embd_layer(missing_box)
                
        for layer in self.layers:

            attended_objs, attn_matrix = layer(detected_obj_feature, missing_token, key_padding_mask=key_padding_mask, \
                                                        detected_obj_pos=detected_obj_pos, missing_obj_pos=missing_obj_pos)

            detected_obj_feature = attended_objs[:,:-1,:]
            missing_token = attended_objs[:,-1:,:]
            
            attn_matrix_list.append(attn_matrix)

        if self.norm is not None:
            output = self.norm(output)
        
        return missing_token, attn_matrix_list 

class ObjInferTransformer_type_2(nn.Module):
    def __init__(self, encoder_layer, num_layers, norm=None, no_pe=False, pe_type='abs4c', no_classifier=True, dim=256, class_numb=200, contain_is_thing=True):
        super().__init__()
        self.layers = _get_clones(encoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm
        self.no_pe = no_pe
        self.pe_type = pe_type
        self.no_classifier = no_classifier
        self.dim = dim
        self.class_numb = class_numb

        self.missing_token = nn.Parameter(torch.randn(1, dim), requires_grad=True)
        self.contain_is_thing = contain_is_thing

        if self.contain_is_thing:
            if self.pe_type in ['abs4c']:
                self.pos_embd_layer = nn.Sequential(nn.Linear(5, int(dim/2)), nn.ReLU(), nn.Linear(int(dim/2), dim), nn.Sigmoid())
            elif self.pe_type in ['abs2c', 'rel2c']:
                self.pos_embd_layer = nn.Sequential(nn.Linear(3, int(dim/2)), nn.ReLU(), nn.Linear(int(dim/2), dim), nn.Sigmoid())

        else:

            if self.pe_type in ['abs4c']:
                self.pos_embd_layer = nn.Sequential(nn.Linear(4, int(dim/2)), nn.ReLU(), nn.Linear(int(dim/2), dim), nn.Sigmoid())
            elif self.pe_type in ['abs2c', 'rel2c']:
                self.pos_embd_layer = nn.Sequential(nn.Linear(2, int(dim/2)), nn.ReLU(), nn.Linear(int(dim/2), dim), nn.Sigmoid())
        
        self.label_embd_layer = nn.Embedding(num_embeddings=self.class_numb, embedding_dim=256)

        if no_classifier:
            self.final_token_prediction = nn.Sequential(*[
                nn.Linear(in_features=self.dim, out_features=self.dim),
                nn.GELU(),
                nn.LayerNorm(self.dim, eps=1e-12)
            ])
            # self.final_bias = nn.Parameter(torch.zeros(128, 128, self.class_numb))

        trunc_normal_(self.missing_token)
        self.apply(init_weights)

    def forward(self, detected_obj_label, bboxes, missing_box, is_thing_label,
                sa_key_padding_mask: Optional[Tensor] = None,
                ca_key_padding_mask: Optional[Tensor] = None):
        
        attn_matrix_list = []

        # detected_obj_feature = detected_obj_label
        detected_obj_feature = self.label_embd_layer(detected_obj_label)

        missing_box = repeat(missing_box, 'B coord -> B 1 coord')
        missing_token = repeat(self.missing_token, 'single E -> batch single E', batch=detected_obj_feature.size(0))

        if self.no_pe:
            detected_obj_pos = None
            missing_obj_pos = None
        else:
            if self.pe_type in ['abs2c', 'rel2c']:
                bboxes = bboxes[:, :, :2]
                missing_box = missing_box[:, :, :2]

                if self.pe_type in ['rel2c']:
                    bboxes -= missing_box
                    missing_box -= missing_box
            
            if self.contain_is_thing:
                bboxes = torch.cat([bboxes, is_thing_label.unsqueeze(dim=-1).type_as(bboxes)], dim=2)
                missing_box = torch.cat([missing_box, torch.tensor([[[1]]]).repeat(missing_box.shape[0],1,1).type_as(missing_box)], dim=2)

            detected_obj_pos = self.pos_embd_layer(bboxes)
            missing_obj_pos = self.pos_embd_layer(missing_box)
        
        for layer in self.layers:
            detected_obj, missing_token, self_attn_score, cross_attn_score = layer(detected_obj_feature, missing_token, sa_key_padding_mask, ca_key_padding_mask, detected_obj_pos=detected_obj_pos, missing_obj_pos=missing_obj_pos)
            attn_matrix_list.append(cross_attn_score)

        if self.norm is not None:
            output = self.norm(output)
        
        return missing_token, attn_matrix_list 

class ObjInferLayer_Type_1(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,
            activation="relu", normalize_before=False):
        super().__init__()
        self.nhead = nhead
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)

        # Implementation of Feedforward model
        self.linear_1 = nn.Linear(d_model, dim_feedforward)
        self.linear_2 = nn.Linear(dim_feedforward, d_model)
        
        self.norm_1 = nn.LayerNorm(d_model)
        self.norm_2 = nn.LayerNorm(d_model)

        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)
        self.dropout_3 = nn.Dropout(dropout)

        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    def forward_post(self, detected_obj, missing_obj_token,
                    key_padding_mask,
                    detected_obj_pos: Optional[Tensor] = None,
                    missing_obj_pos: Optional[Tensor] = None
                    ):

        detected_obj = self.with_pos_embed(detected_obj, detected_obj_pos)
        missing_obj_token = self.with_pos_embed(missing_obj_token, missing_obj_pos)
        src_init=q=k=v=torch.cat([detected_obj, missing_obj_token], dim=1)
        src_1, self_attn_score = self.self_attn(q, k, v, attn_mask=None, key_padding_mask=key_padding_mask)

        ## ADD & Norm
        src_1 = src_init + self.dropout_1(src_1)
        src_1 = self.norm_1(src_1)
        
        ## FFN
        src_2 = self.linear_2(self.dropout_2(self.activation(self.linear_1(src_1))))

        ## ADD & Norm
        src_2 = src_1 + self.dropout_3(src_2)
        src_2 = self.norm_2(src_2)
        
        return src_2, self_attn_score

    def forward_pre(self, detected_obj, missing_obj_token,
                    key_padding_mask,
                    detected_obj_pos: Optional[Tensor] = None,
                    missing_obj_pos: Optional[Tensor] = None
                    ):
        
        ## Norm
        detected_obj, missing_obj_token = self.norm_1(detected_obj), self.norm_1(missing_obj_token)
        detected_obj, missing_obj_token = self.with_pos_embed(detected_obj, detected_obj_pos), self.with_pos_embed(missing_obj_token, missing_obj_pos)

        src_init=q=k=v=self.norm_1(torch.cat([detected_obj, missing_obj_token], dim=1))
        src_1, self_attn_score = self.self_attn(q, k, v, attn_mask=None, key_padding_mask=key_padding_mask)

        ## ADD & Norm
        src_1 = src_init + self.dropout_1(src_1)
        src_1 = self.norm_2(src_1)
        
        ## FFN
        src_2 = self.linear_2(self.dropout_2(self.activation(self.linear_1(src_1))))

        ## ADD
        src_2 = src_1 + self.dropout_3(src_2)

        attended_detected_obj = src_2[:,:,:-1]
        attended_missing_obj_token = src_2[:,:,-1:]

        return attended_detected_obj, attended_missing_obj_token, self_attn_score

    def forward(self,
                detected_obj_feature,
                missing_obj_token,
                key_padding_mask,
                detected_obj_pos: Optional[Tensor] = None,
                missing_obj_pos: Optional[Tensor] = None
                ):

        if self.normalize_before:
            return self.forward_pre(detected_obj_feature, missing_obj_token, key_padding_mask, detected_obj_pos, missing_obj_pos)
        return self.forward_post(detected_obj_feature, missing_obj_token, key_padding_mask, detected_obj_pos, missing_obj_pos)

class ObjInferLayer_Type_2(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,
                activation="relu", normalize_before=False):
        super().__init__()
        self.nhead = nhead

        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
        # Implementation of Feedforward model
        self.linear_1_1 = nn.Linear(d_model, dim_feedforward)
        self.linear_1_2 = nn.Linear(dim_feedforward, d_model)
        
        self.norm_1_1 = nn.LayerNorm(d_model)
        self.norm_1_2 = nn.LayerNorm(d_model)

        self.dropout_1_1 = nn.Dropout(dropout)
        self.dropout_1_2 = nn.Dropout(dropout)
        self.dropout_1_3 = nn.Dropout(dropout)

        # Implementation of Feedforward model
        self.linear_2_1 = nn.Linear(d_model, dim_feedforward)
        self.linear_2_2 = nn.Linear(dim_feedforward, d_model)
        
        self.norm_2_1 = nn.LayerNorm(d_model)
        self.norm_2_2 = nn.LayerNorm(d_model)

        self.dropout_2_1 = nn.Dropout(dropout)
        self.dropout_2_2 = nn.Dropout(dropout)
        self.dropout_2_3 = nn.Dropout(dropout)

        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    def forward_post(self,
                     detected_obj,
                     missing_obj_token,

                     sa_key_padding_mask: Optional[Tensor] = None,
                     ca_key_padding_mask: Optional[Tensor] = None,
                     detected_obj_pos: Optional[Tensor] = None,
                     missing_obj_pos: Optional[Tensor] = None
                     ):
        src_1_init = q_1 = k_1 = v_1 = self.with_pos_embed(detected_obj, detected_obj_pos)
        src_1_1, self_attn_score = self.self_attn(q_1, k_1, v_1, attn_mask=None, key_padding_mask=sa_key_padding_mask)

        ## ADD & Norm
        src_1_1 = src_1_init + self.dropout_1_1(src_1_1)
        src_1_1 = self.norm_1_1(src_1_1)
        
        ## FFN
        src_1_2 = self.linear_1_2(self.dropout_1_2(self.activation(self.linear_1_1(src_1_1))))
        
        ## ADD & Norm
        src_1_2 = src_1_1 + self.dropout_1_3(src_1_2)
        attended_detected_obj = src_1_2 = self.norm_1_2(src_1_2)

        k_2 = v_2 = attended_detected_obj
        src_2_init = q_2 = self.with_pos_embed(missing_obj_token, missing_obj_pos)
        src_2_1, cross_attn_score = self.cross_attn(q_2, k_2, v_2, attn_mask=None, key_padding_mask=ca_key_padding_mask)
        
        ## ADD & Norm
        src_2_1 = src_2_init + self.dropout_2_1(src_2_1)
        src_2_1 = self.norm_2_1(src_2_1)
        
        ## FFN
        src_2_2 = self.linear_2_2(self.dropout_2_2(self.activation(self.linear_2_1(src_2_1))))
        
        ## ADD & Norm
        src_2_2 = src_2_1 + self.dropout_2_3(src_2_2)
        attended_missing_obj_token = src_2_2 = self.norm_2_2(src_2_2)

        return attended_detected_obj, attended_missing_obj_token, self_attn_score, cross_attn_score

    def forward_pre(self,
                     detected_obj,
                     missing_obj_token,

                     detected_obj_mask_self_attn_mask: Optional[Tensor] = None,
                     detected_obj_mask_cross_attn_mask: Optional[Tensor] = None,
                     detected_obj_pos: Optional[Tensor] = None,
                     missing_obj_pos: Optional[Tensor] = None
                     ):
        
        ## Pre Norm
        detected_obj = self.norm_1_1(detected_obj)
        src_1_init = q_1 = k_1 = v_1 = self.with_pos_embed(detected_obj, detected_obj_pos)
        src_1_1, self_attn_score = self.self_attn(q_1, k_1, v_1, attn_mask=detected_obj_mask_self_attn_mask, key_padding_mask=None)

        ## ADD & Norm
        src_1_1 = src_1_init + self.dropout_1_1(src_1_1)
        src_1_1 = self.norm_1_2(src_1_1)

        ## Feed Forward
        src_1_2 = self.linear_1_2(self.dropout_1_2(self.activation(self.linear_1_1(src_1_1))))
        
        ## ADD 
        attended_detected_obj = src_1_2 = src_1_1 + self.dropout_1_3(src_1_2)

        k_2 = v_2 = attended_detected_obj

        ## Pre Norm
        missing_obj_token = self.norm_2_1(missing_obj_token)
        src_2_init = q_2 = self.with_pos_embed(missing_obj_token, missing_obj_pos)
        src_2_1, cross_attn_score = self.cross_attn(q_2, k_2, v_2, attn_mask=detected_obj_mask_cross_attn_mask, key_padding_mask=None)

        ## ADD & Norm
        src_2_1 = src_2_init + self.dropout_2_1(src_2_1)
        src_2_1 = self.norm_2_2(src_2_1)
        
        ## Feed Forward
        src_2_2 = self.linear_2_2(self.dropout_2_2(self.activation(self.linear_2_1(src_2_1))))
        
        ## ADD 
        attended_missing_obj_token = src_2_2 = src_2_1 + self.dropout_2_3(src_2_2)

        return attended_detected_obj, attended_missing_obj_token, self_attn_score, cross_attn_score

    def forward(self,
                detected_obj_feature,
                missing_obj_token,

                sa_key_padding_mask: Optional[Tensor] = None,
                ca_key_padding_mask: Optional[Tensor] = None,
                detected_obj_pos: Optional[Tensor] = None,
                missing_obj_pos: Optional[Tensor] = None
                ):

        if self.normalize_before:
            return self.forward_pre(detected_obj_feature, missing_obj_token, sa_key_padding_mask, ca_key_padding_mask, detected_obj_pos, missing_obj_pos)
        return self.forward_post(detected_obj_feature, missing_obj_token, sa_key_padding_mask, ca_key_padding_mask, detected_obj_pos, missing_obj_pos)